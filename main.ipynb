{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Torch CPU backend.\n"
     ]
    }
   ],
   "source": [
    "from Modules.nn import *\n",
    "\n",
    "hidden_size = 512\n",
    "\n",
    "\n",
    "class ConvNetwork(SequentialNeuralNetwork):\n",
    "    def __init__(self, output_size):\n",
    "        # NOTE: feel free to change structure and seed\n",
    "        sequential = list()\n",
    "        sequential.append(ConvolutionModule(1, 32, window_size=(5, 5), stride=(1, 1), padding=(2, 2)))\n",
    "        sequential.append(ReluModule())\n",
    "        sequential.append(MaxPoolModule(window_size=(2, 2), stride=(2, 2)))\n",
    "        sequential.append(DropoutModule(p=0.15))\n",
    "        sequential.append(ConvolutionModule(32, 64, window_size=(3, 3), stride=(1, 1), padding=(1, 1)))\n",
    "        sequential.append(ReluModule())\n",
    "        sequential.append(MaxPoolModule(window_size=(2, 2), stride=(2, 2)))\n",
    "        sequential.append(DropoutModule(p=0.15))\n",
    "        \n",
    "        sequential.append(LinearModule(7 * 7 * 64, hidden_size))\n",
    "        sequential.append(ReluModule())\n",
    "        sequential.append(LinearModule(hidden_size, hidden_size))\n",
    "        sequential.append(ReluModule())\n",
    "        sequential.append(DropoutModule(p=0.25))\n",
    "        sequential.append(LinearModule(hidden_size, hidden_size))\n",
    "        sequential.append(ReluModule())\n",
    "        sequential.append(LinearModule(hidden_size, hidden_size))\n",
    "        sequential.append(ReluModule())\n",
    "        sequential.append(DropoutModule(p=0.25))\n",
    "        sequential.append(LinearModule(hidden_size, output_size))\n",
    "\n",
    "        loss_func = CrossEntropyLoss()\n",
    "        optimizer = Adam(lr=1e-3)\n",
    "        super().__init__(sequential, loss_func, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from utils.mnist import read_data_sets\n",
    "from utils.one_hot import encode_one_hot\n",
    "import sys\n",
    "\n",
    "batch_size = 100\n",
    "display_size = 10000\n",
    "shuffle = True\n",
    "seed = 20210527\n",
    "\n",
    "f0 = sys.stdout\n",
    "f = open('./result/' + \"batch_size{}_seed{}_console_rec.txt\".format(batch_size, seed), 'w')\n",
    "sys.stdout = f0\n",
    "\n",
    "# Set Seed\n",
    "np.random.seed(seed)\n",
    "\n",
    "# read data\n",
    "train_data, _, test_data = read_data_sets('./data/', val_size=0)\n",
    "x_train, x_test = train_data[0] * 2 - 1, test_data[0] * 2 - 1\n",
    "y_train, y_test = train_data[1], test_data[1]\n",
    "\n",
    "# access data size: num_samples & num_features\n",
    "num_samples, height, width, num_channels = x_train.shape\n",
    "num_features = height * width\n",
    "num_classes = 10\n",
    "\n",
    "# Initialize\n",
    "# model = LinearNetwork(num_features, num_classes)\n",
    "model = ConvNetwork(num_classes)\n",
    "\n",
    "# encode one-hot vector\n",
    "y_01 = encode_one_hot(y_train)\n",
    "\n",
    "start = time.time()\n",
    "num_batches = num_samples // batch_size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Sample [10000/60000], Loss: 1.61177\n",
      "Epoch [1/20], Sample [20000/60000], Loss: 0.43902\n",
      "Epoch [1/20], Sample [30000/60000], Loss: 0.19693\n",
      "Epoch [1/20], Sample [40000/60000], Loss: 0.15055\n",
      "Epoch [1/20], Sample [50000/60000], Loss: 0.12691\n",
      "Epoch [1/20], Sample [60000/60000], Loss: 0.11382\n",
      "Epoch [2/20], Sample [10000/60000], Loss: 0.08851\n",
      "Epoch [2/20], Sample [20000/60000], Loss: 0.09142\n",
      "Epoch [2/20], Sample [30000/60000], Loss: 0.08890\n",
      "Epoch [2/20], Sample [40000/60000], Loss: 0.08841\n",
      "Epoch [2/20], Sample [50000/60000], Loss: 0.08449\n",
      "Epoch [2/20], Sample [60000/60000], Loss: 0.07396\n",
      "Epoch [3/20], Sample [10000/60000], Loss: 0.06873\n",
      "Epoch [3/20], Sample [20000/60000], Loss: 0.07494\n",
      "Epoch [3/20], Sample [30000/60000], Loss: 0.06140\n",
      "Epoch [3/20], Sample [40000/60000], Loss: 0.06346\n",
      "Epoch [3/20], Sample [50000/60000], Loss: 0.05423\n",
      "Epoch [3/20], Sample [60000/60000], Loss: 0.06509\n",
      "Epoch [4/20], Sample [10000/60000], Loss: 0.04406\n",
      "Epoch [4/20], Sample [20000/60000], Loss: 0.05203\n",
      "Epoch [4/20], Sample [30000/60000], Loss: 0.04915\n",
      "Epoch [4/20], Sample [40000/60000], Loss: 0.06935\n",
      "Epoch [4/20], Sample [50000/60000], Loss: 0.06042\n",
      "Epoch [4/20], Sample [60000/60000], Loss: 0.05051\n",
      "Epoch [5/20], Sample [10000/60000], Loss: 0.04066\n",
      "Epoch [5/20], Sample [20000/60000], Loss: 0.04209\n",
      "Epoch [5/20], Sample [30000/60000], Loss: 0.04984\n",
      "Epoch [5/20], Sample [40000/60000], Loss: 0.04649\n",
      "Epoch [5/20], Sample [50000/60000], Loss: 0.04681\n",
      "Epoch [5/20], Sample [60000/60000], Loss: 0.05075\n",
      "Epoch [6/20], Sample [10000/60000], Loss: 0.03946\n",
      "Epoch [6/20], Sample [20000/60000], Loss: 0.04077\n",
      "Epoch [6/20], Sample [30000/60000], Loss: 0.04475\n",
      "Epoch [6/20], Sample [40000/60000], Loss: 0.04809\n",
      "Epoch [6/20], Sample [50000/60000], Loss: 0.03879\n",
      "Epoch [6/20], Sample [60000/60000], Loss: 0.04901\n",
      "Epoch [7/20], Sample [10000/60000], Loss: 0.03789\n",
      "Epoch [7/20], Sample [20000/60000], Loss: 0.03656\n",
      "Epoch [7/20], Sample [30000/60000], Loss: 0.02980\n",
      "Epoch [7/20], Sample [40000/60000], Loss: 0.03855\n",
      "Epoch [7/20], Sample [50000/60000], Loss: 0.03047\n",
      "Epoch [7/20], Sample [60000/60000], Loss: 0.03846\n",
      "Epoch [8/20], Sample [10000/60000], Loss: 0.03290\n",
      "Epoch [8/20], Sample [20000/60000], Loss: 0.02918\n",
      "Epoch [8/20], Sample [30000/60000], Loss: 0.03372\n",
      "Epoch [8/20], Sample [40000/60000], Loss: 0.03584\n",
      "Epoch [8/20], Sample [50000/60000], Loss: 0.03093\n",
      "Epoch [8/20], Sample [60000/60000], Loss: 0.04118\n",
      "Epoch [9/20], Sample [10000/60000], Loss: 0.02547\n",
      "Epoch [9/20], Sample [20000/60000], Loss: 0.02666\n",
      "Epoch [9/20], Sample [30000/60000], Loss: 0.02880\n",
      "Epoch [9/20], Sample [40000/60000], Loss: 0.03186\n",
      "Epoch [9/20], Sample [50000/60000], Loss: 0.03348\n",
      "Epoch [9/20], Sample [60000/60000], Loss: 0.03412\n",
      "Epoch [10/20], Sample [10000/60000], Loss: 0.02376\n",
      "Epoch [10/20], Sample [20000/60000], Loss: 0.02887\n",
      "Epoch [10/20], Sample [30000/60000], Loss: 0.02919\n",
      "Epoch [10/20], Sample [40000/60000], Loss: 0.02202\n",
      "Epoch [10/20], Sample [50000/60000], Loss: 0.04487\n",
      "Epoch [10/20], Sample [60000/60000], Loss: 0.03170\n",
      "Epoch [11/20], Sample [10000/60000], Loss: 0.02745\n",
      "Epoch [11/20], Sample [20000/60000], Loss: 0.02042\n",
      "Epoch [11/20], Sample [30000/60000], Loss: 0.02741\n",
      "Epoch [11/20], Sample [40000/60000], Loss: 0.02562\n",
      "Epoch [11/20], Sample [50000/60000], Loss: 0.02486\n",
      "Epoch [11/20], Sample [60000/60000], Loss: 0.02884\n",
      "Epoch [12/20], Sample [10000/60000], Loss: 0.02731\n",
      "Epoch [12/20], Sample [20000/60000], Loss: 0.02431\n",
      "Epoch [12/20], Sample [30000/60000], Loss: 0.02884\n",
      "Epoch [12/20], Sample [40000/60000], Loss: 0.02968\n",
      "Epoch [12/20], Sample [50000/60000], Loss: 0.02957\n",
      "Epoch [12/20], Sample [60000/60000], Loss: 0.02191\n",
      "Epoch [13/20], Sample [10000/60000], Loss: 0.01636\n",
      "Epoch [13/20], Sample [20000/60000], Loss: 0.02723\n",
      "Epoch [13/20], Sample [30000/60000], Loss: 0.01723\n",
      "Epoch [13/20], Sample [40000/60000], Loss: 0.02854\n",
      "Epoch [13/20], Sample [50000/60000], Loss: 0.02529\n",
      "Epoch [13/20], Sample [60000/60000], Loss: 0.02668\n",
      "Epoch [14/20], Sample [10000/60000], Loss: 0.01783\n",
      "Epoch [14/20], Sample [20000/60000], Loss: 0.01880\n",
      "Epoch [14/20], Sample [30000/60000], Loss: 0.01887\n",
      "Epoch [14/20], Sample [40000/60000], Loss: 0.02776\n",
      "Epoch [14/20], Sample [50000/60000], Loss: 0.02027\n",
      "Epoch [14/20], Sample [60000/60000], Loss: 0.02215\n",
      "Epoch [15/20], Sample [10000/60000], Loss: 0.02172\n",
      "Epoch [15/20], Sample [20000/60000], Loss: 0.02296\n",
      "Epoch [15/20], Sample [30000/60000], Loss: 0.02504\n",
      "Epoch [15/20], Sample [40000/60000], Loss: 0.01781\n",
      "Epoch [15/20], Sample [50000/60000], Loss: 0.01727\n",
      "Epoch [15/20], Sample [60000/60000], Loss: 0.02076\n",
      "Epoch [16/20], Sample [10000/60000], Loss: 0.01912\n",
      "Epoch [16/20], Sample [20000/60000], Loss: 0.02561\n",
      "Epoch [16/20], Sample [30000/60000], Loss: 0.01675\n",
      "Epoch [16/20], Sample [40000/60000], Loss: 0.01623\n",
      "Epoch [16/20], Sample [50000/60000], Loss: 0.03518\n",
      "Epoch [16/20], Sample [60000/60000], Loss: 0.02485\n",
      "Epoch [17/20], Sample [10000/60000], Loss: 0.01750\n",
      "Epoch [17/20], Sample [20000/60000], Loss: 0.02242\n",
      "Epoch [17/20], Sample [30000/60000], Loss: 0.01952\n",
      "Epoch [17/20], Sample [40000/60000], Loss: 0.01227\n",
      "Epoch [17/20], Sample [50000/60000], Loss: 0.02405\n",
      "Epoch [17/20], Sample [60000/60000], Loss: 0.01684\n",
      "Epoch [18/20], Sample [10000/60000], Loss: 0.02116\n",
      "Epoch [18/20], Sample [20000/60000], Loss: 0.01527\n",
      "Epoch [18/20], Sample [30000/60000], Loss: 0.01494\n",
      "Epoch [18/20], Sample [40000/60000], Loss: 0.01447\n",
      "Epoch [18/20], Sample [50000/60000], Loss: 0.01145\n",
      "Epoch [18/20], Sample [60000/60000], Loss: 0.02772\n",
      "Epoch [19/20], Sample [10000/60000], Loss: 0.02389\n",
      "Epoch [19/20], Sample [20000/60000], Loss: 0.01988\n",
      "Epoch [19/20], Sample [30000/60000], Loss: 0.01596\n",
      "Epoch [19/20], Sample [40000/60000], Loss: 0.01573\n",
      "Epoch [19/20], Sample [50000/60000], Loss: 0.01636\n",
      "Epoch [19/20], Sample [60000/60000], Loss: 0.01989\n",
      "Epoch [20/20], Sample [10000/60000], Loss: 0.01226\n",
      "Epoch [20/20], Sample [20000/60000], Loss: 0.01512\n",
      "Epoch [20/20], Sample [30000/60000], Loss: 0.01353\n",
      "Epoch [20/20], Sample [40000/60000], Loss: 0.01710\n",
      "Epoch [20/20], Sample [50000/60000], Loss: 0.02534\n",
      "Epoch [20/20], Sample [60000/60000], Loss: 0.02037\n",
      "Training Time:12995.835790157318 seconds\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "num_epochs = 20\n",
    "start = time.time()\n",
    "num_batches = num_samples // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    loss = 0\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(num_samples))\n",
    "        x_train = x_train[shuffle_indices, :]\n",
    "        y_01 = y_01[shuffle_indices, :]\n",
    "    for batch in range(num_batches):\n",
    "        batch_idc = range(batch * batch_size, (batch + 1) * batch_size)\n",
    "        loss += model.train(x_train[batch_idc, :], y_01[batch_idc, :])\n",
    "        if ((batch+1) * batch_size) % display_size == 0:\n",
    "            print('Epoch [{}/{}], Sample [{}/{}], Loss: {:.5f}'\n",
    "                  .format(epoch + 1, num_epochs, (batch+1) * batch_size, num_samples, loss/display_size))\n",
    "            loss = 0\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('Training Time:{} seconds'.format(end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing.............\n",
      "10000 Test Samples Accuracy:0.9933\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "correct = 0\n",
    "print('Testing...', end=\"\")\n",
    "num_test_samples = x_test.shape[0]\n",
    "for batch in range(num_test_samples // batch_size):\n",
    "    batch_idc = range(batch * batch_size, (batch + 1) * batch_size)\n",
    "    y_hat = model.predict(x_test[batch_idc, :])\n",
    "    correct += np.sum(y_hat == y_test[batch_idc])\n",
    "    if ((batch + 1) * batch_size) % (num_test_samples // 10) == 0:\n",
    "        print('.', end=\"\")\n",
    "print('\\n{} Test Samples Accuracy:{}'.format(num_test_samples, correct / num_test_samples))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
